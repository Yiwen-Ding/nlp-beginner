{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类——pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import init\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文件数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, sent_col_name, label_col_name):\n",
    "    data = pd.read_csv(file_path, sep = \"\\t\")\n",
    "    X = data[sent_col_name].values\n",
    "    y = data[label_col_name].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_df, val_df = pd.DataFrame(), pd.DataFrame()\n",
    "    train_df[\"Phrase\"], train_df[\"Sentiment\"] = X_train, y_train\n",
    "    val_df[\"Phrase\"],val_df[\"Sentiment\"] = X_val,y_val\n",
    "\n",
    "    train_df_path = \"data/train.csv\"\n",
    "    val_df_path = \"data/val.csv\"\n",
    "    train_df.to_csv(train_df_path, index = False)\n",
    "    val_df.to_csv(val_df_path, index = False)\n",
    "\n",
    "    return train_df_path, val_df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size):\n",
    "    X_col_name = \"Phrase\"\n",
    "    y_col_name = \"Sentiment\"\n",
    "    train_path = \"data/train.tsv\"\n",
    "    train_df_path, val_df_path = read_data(train_path,X_col_name,y_col_name)\n",
    "    \n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenizer(text): # create a tokenizer function\n",
    "        \"\"\"\n",
    "        定义分词操作\n",
    "        \"\"\"\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    TEXT = data.Field(sequential = True, tokenize = tokenizer, lower = True)\n",
    "    LABEL = data.Field(sequential = False, use_vocab = False)\n",
    "    # TabularDataset: 用来从文件中读取数据，生成Dataset， Dataset是Example实例的集合\n",
    "    train, val = data.TabularDataset.splits(\n",
    "        path='', train=train_df_path, validation=val_df_path, format='csv', skip_header=True,\n",
    "        fields=[(\"Phrase\", TEXT), (\"Sentiment\", LABEL)])\n",
    "\n",
    "    # 使用训练集构建单词表\n",
    "    TEXT.build_vocab(train, vectors='glove.6B.50d')\n",
    "    TEXT.vocab.vectors.unk_init = init.xavier_uniform\n",
    "    print(type(TEXT))\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 生成数据迭代器\n",
    "    train_iter = data.BucketIterator(train, batch_size = batch_size, \n",
    "                        sort_key = lambda x: len(x.review), device = DEVICE)\n",
    "    val_iter = data.BucketIterator(val, batch_size = batch_size, \n",
    "                        sort_key = lambda x: len(x.review), shuffle = True, device = DEVICE)\n",
    "    \n",
    "    print(type(train_iter))\n",
    "    return train_iter, val_iter, TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, class_num, filter_num, filter_size, dropout_p):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)\n",
    "        # 卷积层\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = filter_num,\n",
    "                            kernel_size = (filter_size[0], embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 1, out_channels = filter_num,\n",
    "                            kernel_size = (filter_size[1], embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels = 1, out_channels = filter_num,\n",
    "                            kernel_size = (filter_size[2], embedding_dim))\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(3 * filter_num, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的维度为(Batch_size, Length) \n",
    "        # (Batch_size, Length, Dimention) \n",
    "        # 增加维度后(Batch_size, 1, Length, Dimention) \n",
    "        embed_out = self.embed(x).unsqueeze(1)\n",
    "\n",
    "        #(Batch_size, filter_num, length+padding, 1) \n",
    "        # 降低维度后(Batch_size, filter_num, length+padding) \n",
    "        conv1_out = F.relu(self.conv1(embed_out)).squeeze(3)\n",
    "\n",
    "        #(Batch_size, filters_num, 1)\n",
    "        # 降低维度后(Batch_size, filters_num) \n",
    "        pool1_out = F.max_pool1d(conv1_out, conv1_out.size(2)).squeeze(2)\n",
    "\n",
    "        conv2_out = F.relu(self.conv2(embed_out)).squeeze(3)\n",
    "        pool2_out = F.max_pool1d(conv2_out, conv2_out.size(2)).squeeze(2)\n",
    "\n",
    "        conv3_out = F.relu(self.conv3(embed_out)).squeeze(3)\n",
    "        pool3_out = F.max_pool1d(conv2_out, conv3_out.size(2)).squeeze(2)\n",
    "\n",
    "        # (Batch_size, filters_num *3 )\n",
    "        out_cat = torch.cat((pool1_out,pool2_out,pool3_out), dim = 1)\n",
    "        out_cat = self.dropout(out_cat)\n",
    "        out = self.fc(out_cat)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, class_num, \n",
    "                    rnn_type, hidden_size,dropout_p,num_layers):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size = embedding_dim, num_layers = num_layers,\n",
    "                            hidden_size = hidden_size, batch_first=True,dropout = dropout_p)\n",
    "            #self.dropout = nn.Dropout(dropout_p)    \n",
    "            self.fc = nn.Linear(hidden_size, class_num)\n",
    "        elif rnn_type == \"LSTM\":\n",
    "             # 双向传播\n",
    "            self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, num_layers = num_layers,\n",
    "                            batch_first=True, bidirectional=True,dropout = dropout_p)\n",
    "            #self.dropout = nn.Dropout(dropout_p)    \n",
    "            self.fc = nn.Linear(hidden_size * 2, class_num)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input_sents (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.shape\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        embed_out = self.embed(x)\n",
    "\n",
    "        if self.rnn_type == \"RNN\":\n",
    "            h0 = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "            output, hn = self.rnn(embed_out, h0)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            h0, c0 = torch.randn(self.num_layers * 2, batch_size, self.hidden_size), torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "            output, (hn, _) = self.lstm(embed_out, (h0, c0))\n",
    "\n",
    "        #print(output.shape,output[:, -1, :].shape)\n",
    "        \n",
    "        out = self.fc(output[:, -1, :]) \n",
    "        #print(out.shape)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = [\"RNN\",\"LSTM\", \"CNN\"]\n",
    "model_names = [\"CNN\"]\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epoch_num = 10\n",
    "class_num = 5\n",
    "embedding_dim = 50\n",
    "filter_num = 100\n",
    "hidden_size = 50\n",
    "dropout_p = 0.2\n",
    "num_layers=2  #层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\cs224n\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "D:\\Anaconda\\envs\\cs224n\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training average accuracy: 53.840%, Training average Loss: 1.155302,Validation average accuracy: 57.765%\n",
      "Epoch 1: Training average accuracy: 60.090%, Training average Loss: 0.994987,Validation average accuracy: 61.290%\n",
      "Epoch 2: Training average accuracy: 64.119%, Training average Loss: 0.893090,Validation average accuracy: 63.049%\n",
      "Epoch 3: Training average accuracy: 66.458%, Training average Loss: 0.827922,Validation average accuracy: 64.675%\n",
      "Epoch 4: Training average accuracy: 68.327%, Training average Loss: 0.781886,Validation average accuracy: 65.238%\n",
      "Epoch 5: Training average accuracy: 69.616%, Training average Loss: 0.747658,Validation average accuracy: 65.126%\n",
      "Epoch 6: Training average accuracy: 70.775%, Training average Loss: 0.720210,Validation average accuracy: 66.119%\n",
      "Epoch 7: Training average accuracy: 71.493%, Training average Loss: 0.698419,Validation average accuracy: 66.142%\n",
      "Epoch 8: Training average accuracy: 72.484%, Training average Loss: 0.678371,Validation average accuracy: 65.771%\n",
      "Epoch 9: Training average accuracy: 72.959%, Training average Loss: 0.659708,Validation average accuracy: 66.220%\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, word_vectors = data_loader(batch_size = batch_size)\n",
    "# 三种模型轮流训练\n",
    "for model_name in model_names:\n",
    "    if model_name == \"RNN\":\n",
    "        model = TextRNN(vocab_size = len(word_vectors), embedding_dim = embedding_dim, \n",
    "                    rnn_type = \"RNN\",hidden_size = hidden_size, class_num = class_num,dropout_p = dropout_p,num_layers = num_layers)\n",
    "    elif model_name == \"CNN\":\n",
    "        model = TextCNN(vocab_size=len(word_vectors), embedding_dim = embedding_dim, \n",
    "                    class_num = class_num, filter_num = filter_num, filter_size = [3, 4, 5],dropout_p = dropout_p)\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = TextRNN(vocab_size = len(word_vectors), embedding_dim = embedding_dim, \n",
    "                    rnn_type = \"LSTM\",hidden_size = hidden_size, class_num = class_num,dropout_p = dropout_p, num_layers = num_layers)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)# 创建优化器SGD\n",
    "    criterion = nn.CrossEntropyLoss()   # 损失函数\n",
    "\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x, y = batch.Phrase.t(), batch.Sentiment\n",
    "            optimizer.zero_grad() # 梯度缓存清零\n",
    "            predict = model(x)\n",
    "            loss = criterion(predict, y)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()         # 反向传播\n",
    "            optimizer.step()\n",
    "            acc = torch.mean((torch.tensor(torch.max(predict,1)[1] == y, dtype=torch.float)))\n",
    "            train_accs.append(acc)\n",
    "            #total_correct = total_correct + correct.item()\n",
    "        train_acc = np.array(train_accs).mean() * 100\n",
    "        train_loss = np.array(train_loss).mean()\n",
    "\n",
    "        model.eval()\n",
    "        val_accs = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            x, y = batch.Phrase.t(), batch.Sentiment\n",
    "            predict = model(x)\n",
    "            acc = torch.mean((torch.tensor(torch.max(predict,1)[1] == y, dtype=torch.float)))\n",
    "            val_accs.append(acc)\n",
    "        val_acc = np.array(val_accs).mean() * 100\n",
    "\n",
    "        print(\"Epoch %d: Training average accuracy: %.3f%%, Training average Loss: %f,Validation average accuracy: %.3f%%\"\n",
    "                %(epoch, train_acc ,train_loss,val_acc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17b984ac83de4acb5c5ea27d9e8ebbfb95551cd9e509646bdc72d92dd2017a1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('cs224n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
